<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-type" content="text/html; charset=utf-8"/>

    
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
    <meta http-equiv="Pragma" content="no-cache" />
    <meta http-equiv="Expires" content="0" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

    <!-- ICONS -->
    <script src="https://kit.fontawesome.com/bacac70704.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <link rel="shortcut icon" href="resrc/icons/Romain_thumb.png">

    <meta name="description" content="Romain Ilbert">
    <meta name="author" content="Romain Ilbert">

    <title>Romain Ilbert </title>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-iYQeCzEYFbKjA/T2uDLTpkwGzCiq6soy8tYaI1GyVh/UjpbCx/TYkiZhlZB6+fzT" crossorigin="anonymous">
    <link href="css_files/style.css" rel="stylesheet">
    <link href="css_files/media.css" rel="stylesheet">
    <!-- Core theme JS-->
    <script src="scripts.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-60DVFC3G0P"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-60DVFC3G0P');
  </script>

  </head>

  <body>

    <div class="container">
      <div class="row">
        <div class="col-md-8 col-sm-8 col-xs-12">
	  <br>
          <h1>Romain Ilbert</h1>
          <div class="dark-mode-toggle">
            <button onclick="toggleDarkMode()" type="button" aria-label="Toggle dark mode" class="group rounded-full bg-white/90 px-3 py-2 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur transition dark:bg-zinc-800/90 dark:ring-white/10 dark:hover:ring-white/20">
                <!-- Soleil avec des rayons (visible en mode clair) -->
                <svg viewBox="0 0 24 24" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true" class="h-6 w-6 fill-zinc-100 stroke-zinc-500 transition group-hover:fill-zinc-200 group-hover:stroke-zinc-700 dark:hidden">
                  <!-- Le cercle central du soleil -->
                  <circle cx="12" cy="12" r="5"></circle>
                  <!-- Rayons autour du soleil -->
                  <line x1="12" y1="1" x2="12" y2="3"></line>
                  <line x1="12" y1="21" x2="12" y2="23"></line>
                  <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                  <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                  <line x1="1" y1="12" x2="3" y2="12"></line>
                  <line x1="21" y1="12" x2="23" y2="12"></line>
                  <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                  <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                </svg>
                <!-- Lune (visible en mode sombre) -->
                <!-- IcÃ´ne lune -->
                <svg viewBox="0 0 24 24" aria-hidden="true" class="hidden h-6 w-6 fill-zinc-700 stroke-zinc-500 transition dark:block">
                  <path d="M17.25 16.22a6.937 6.937 0 0 1-9.47-9.47 7.451 7.451 0 1 0 9.47 9.47Z"></path>
                  <path d="M12.75 7C17 7 17 2.75 17 2.75S17 7 21.25 7C17 7 17 11.25 17 11.25S17 7 12.75 7Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path>
                </svg>
            </button>
          </div>

          <h3 class="custom">
            Incoming Research Scientist <img src="meta/meta_logo.svg" alt="Meta" style="height: 24px; vertical-align: middle; margin-left: 8px;">
          </h3>
          <br>
          <p style="font-size: 120%;text-align: justify;">
          </p>

          <div id="contact_phd">
            <p style="font-size: 120%; text-align: justify;">
              ðŸŽ“ I successfully defended my Ph.D. in May 2025. <br>
              <br> My thesis was conducted jointly between the <b><a href="http://lipade.mi.parisdescartes.fr" target="_blank">LIPADE Research Lab</a></b> (UniversitÃ© Paris CitÃ©) and the <b><a href="https://www.huawei.com/fr/publications/huawei-research" target="_blank">Noah's Ark Lab</a></b> in Paris. <br>
              <br>
              I'm currently transitioning to a new Research Scientist position.  
            </p>
          </div>

        </div>
        <div class="col-md-4 col-sm-4 col-xs-12 text-center">
          <br>
	  <br>
	  <br>
          <img src="romain_acm_ccs.jpg" alt="Romain Ilbert" style="width: 100%;max-width:300px; border-radius: 20px;">
	  <br>
	  <h5 style="padding-top: 5px;">
            <a href="mailto:romain.ilbert@hotmail.fr" title="e-Mail" target="_blank"><i class="fa fa-envelope-square fa-3x"></i></a>
            <a href="https://github.com/romilbert" title="GitHub" target="_blank"><i class="fa fa-github-square fa-3x"></i></a>
            <a href="https://www.linkedin.com/in/romain-ilbert/" title="LinkedIn" target="_blank"><i class="fa fa-linkedin-square fa-3x"></i></a>
	    <a href="https://scholar.google.com/citations?user=65uE37cAAAAJ&hl=en" title="Google Scholar" target="_blank">
	      <i class="ai ai-google-scholar-square ai-3x"></i>
	    </a>
	    <a href="https://arxiv.org/search/?query=Romain+Ilbert&searchtype=all&abstracts=show&order=-announced_date_first&size=50" target="_blank">
	      <i class="ai ai-arxiv-square ai-3x"></i>
	    </a>
	    <a href="CV_Romain_Juin_2025.pdf" title="CV" target="_blank"><i class="ai ai-cv-square ai-3x"></i></a>
	  </h5>
        </div>
      </div>


      <div class="row">
        <div class="col-md-12">
          <h3>Introduction</h3>
        <p style="font-size: 120%;text-align: justify;">
          <p style="font-size: 120%; text-align: justify;">
      I hold an engineering diploma in Data Science, Statistics, and Learning from <a href="https://www.ensae.fr/en" target="_blank">ENSAE</a> and a Master's degree in Machine Learning Research from <a href="https://www.ip-paris.fr/en/education/masters/applied-mathematics-and-statistics-program/master-year-2-data-science" target="_blank">Ã‰cole Polytechnique</a>. 
      Currently, I am a CIFRE PhD student working with the LIPADE lab at Paris Descartes University and the Noah's Ark Lab at Huawei Technologies France, co-advised by <a href="https://ievred.github.io" target="_blank">Ievgen Redko</a> and <a href="https://helios2.mi.parisdescartes.fr/~themisp/" target="_blank">Themis Palpanas</a>.            
      My research focuses extensively on time series analysis, with a particular emphasis on classification and forecasting tasks. 
      In the initial phase of my doctoral research, I explored methods for data augmentation and the synthetic generation of time series to address the challenges of extensive labeling and data cleaning. 
      Progressing further, my focus shifted towards adversarial attacks, where I developed cutting-edge attack strategies alongside robust defense mechanisms. 
      Subsequently, I focused on time series forecasting, particularly investigating the limitations of transformers in this context and strategizing on enhancing their performance. 
      This effort led to the development of SAMformer, a new state-of-the-art model in time series forecasting, excelling in both performance and training time. 
      To enhance the performance of univariate time series forecasters I developed a regularization technique along with a random matrix theory analysis, making a univariate linear model state-of-the-art in multivariate time series forecasting. 
      This paper has been accepted as a Spotlight at NeurIPS 2024, and its Time Series variant has been accepted at the NeurIPS workshop Time Series in the Age of Large Models.
      Currently, my research is centered on the development of foundation models for multivariate time series classification. My work, while centered on time series classification and forecasting, can be readily adapted to other fields like computer vision or NLP.
      In total, I have had 5 papers accepted during my PhD from now. These papers are detailed in the last section of this page.<br>

      <br><i>Note: Due to my involvement in a business-oriented team during the first year and a half of my PhD, my initial work could not lead to publications, owing to data confidentiality and contractual clauses.</i>
      
        </p>
    

        </div>
      </div>

      <div class="row">
        <div class="col-md-12">
          <h3>News</h3>
          <ul class="mb-0" style="list-style-type:none;padding-left:0;font-size: 120%;">
      <li><span class="badge badge-2025" style="width: 85px">06/2025</span> ðŸ“„ <b><a href="https://arxiv.org/pdf/2502.15637?" target="_blank">Mantis</a></b> has been accepted at the ICML 2025 Workshop on Foundation Models for Structured Data.</li>
      <li><span class="badge badge-2025" style="width: 85px">05/2025</span> I presented the paper <b><a href="adapters/adapters_multisa.pdf" target="_blank">User-Friendly Foundation Model Adapters for Multivariate Time Series Classification</a></b> at the ICDE conference in Hong Kong.</li>
      <li><span class="badge badge-2025" style="width: 85px">05/2025</span> ðŸŽ“ I successfully defended my Ph.D. You can find my thesis <a href="thesis/romain_ilbert_phd.pdf" target="_blank">here</a>.</li>
      <li><span class="badge badge-2025" style="width: 85px">04/2025</span> ðŸ“„ <b><a href="https://arxiv.org/pdf/2502.15637?" target="_blank">Mantis: Lightweight Calibrated Foundation Model for User-friendly Time Series Classification</a></b> was accepted as the only long oral paper on time series at CAP 2025.</li>
      <li><span class="badge badge-2025" style="width: 85px">03/2025</span> ðŸ“„ <b><a href="adapters/adapters_multisa.pdf" target="_blank">User-friendly Foundation Model Adapters for Multivariate Time Series Classification</a></b> has been accepted to Multisa, an ICDE workshop.</li><br>
      <li><span class="badge bg-primary" style="width: 85px">12/2024</span> I attended NeurIPS and the NeurIPS workshop "Time Series in the Age of Large Models" to present my two papers in Vancouver.</li>
      <li><span class="badge bg-primary" style="width: 85px">10/2024</span> ðŸ“„ <b><a href="https://openreview.net/forum?id=YaErRZ5SVG&referrer=%5Bthe%20profile%20of%20Romain%20Ilbert%5D(%2Fprofile%3Fid%3D~Romain_Ilbert1)"> Enhancing Multivariate Time Series Forecasting via Multi-Task Learning and Random Matrix Theory</a></b> has been accepted to the NeurIPS workshop "Time Series in the Age of Large Models".</li>
      <li><span class="badge bg-primary" style="width: 85px">09/2024</span>  <b><a href="https://arxiv.org/pdf/2409.12264">User-friendly Foundation Model Adapters for Multivariate Time Series Classification</a></b> is now on arXiv.</li>
      <li><span class="badge bg-primary" style="width: 85px">09/2024</span> ðŸ“„ <b><a href="https://arxiv.org/pdf/2406.10327">Analysing Multi-Task Regression via Random Matrix Theory with Application to Time Series Forecasting</a></b> has been accepted as a Spotlight at NeurIPS 2024.</li>
      <li><span class="badge bg-primary" style="width: 85px">07/2024</span>  I presented SAMformer in ICML in Vienna. You can find the slides <a href="samformer_slides_ICML_final_pdf.pdf">here</a> </li>
      <li><span class="badge bg-primary" style="width: 85px">06/2024</span>  I presented SAMformer in Cap in Lille. You can find the slides <a href="1473_samformer_slides_cap_ILBERT.pdf">here</a> </li>
      <li><span class="badge bg-primary" style="width: 85px">05/2024</span> ðŸ“„ <b><a href="https://arxiv.org/abs/2402.10198v2">SAMformer</a></b> has been accepted as an oral presentation at ICML 2024. You can find my corresponding code on my <a href="https://github.com/romilbert"></a>Github</a>. As the lead on this project, I was responsible for the architecture design, code implementation and all experiments presented in the paper. I also want to thank my co-authors for their assistance in writing the paper.</li>
      <li><span class="badge bg-primary" style="width: 85px">05/2024</span>  I presented Data Augmentation for Multivariate Time Series Classification: An Experimental Study at the ICDE conference in Utrecht</li>
      <li><span class="badge bg-primary" style="width: 85px">04/2024</span> ðŸ“„ <b><a href="https://arxiv.org/abs/2406.06518">Data Augmentation for Multivariate Time Series Classification: An Experimental Study</a></b> has been accepted to Multisa, an ICDE workshop</li>
      <li><span class="badge bg-primary" style="width: 85px">04/2024</span> Starting to work on Multi-Task Learning : From Univariate to Multivariate Time Series Forecasting</li>
      <li><span class="badge bg-primary" style="width: 85px">02/2024</span> My new paper leveraging <b></b><a href="https://arxiv.org/abs/2402.10198v2">SAMformer</a></b>, a new lightweight state-of-the-art multivariate time series forecasting model, is now on arXiv</li><br>
      <li><span class="badge badge-2023" style="width: 85px">12/2023</span> I've attended the <a href="https://neuripsinparis.github.io/neurips2023paris/">NeurIPS in Paris</a> conference</li>
      <li><span class="badge badge-2023" style="width: 85px">11/2023</span> I've <a href="Ilbert_ARTMAN_presentation.pdf">presented</a> Breaking Boundaries at the ARTMAN workshop of the ACM CCS conference (top conference in cybersecurity)</li>
      <li><span class="badge badge-2023" style="width: 85px">10/2023</span> Starting to work on Time Series Forecasting</li>
      <li><span class="badge badge-2023" style="width: 85px">09/2023</span> Starting to work with the <a href="https://www.noahlab.com.hk/#/home">Noah's Ark Lab</a> under the supervision of <a href="https://ievred.github.io">Ievgen Redko</a></li>
	    <li><span class="badge badge-2023" style="width: 85px">08/2023</span> ðŸ“„ <b><a href="https://arxiv.org/abs/2311.09790">Breaking Boundaries</a></b> paper has been accepted to ARTMAN 2023, an <a href="https://www.sigsac.org/ccs/CCS2023/" target="_blank">ACM-CCS</a> Workshop </li> 
	    <li><span class="badge badge-2023" style="width: 85px">07/2023</span> I attended the <a href="https://acdl2023.icas.cc/program/">ACDL</a></b> Summer School </li>
      <li><span class="badge badge-2023" style="width: 85px">01/2023</span> Starting to work on Adversarial Machine learning </li><br>
      <li><span class="badge badge-2022" style="width: 85px">04/2022</span> Starting to work on Data Augmentation for Time Series Classification </li>
	    <li><span class="badge badge-2022" style="width: 85px">04/2022</span> Starting of my PhD with the <a href="http://lipade.mi.parisdescartes.fr/" target="_blank">Lipade research Lab</a>  </li><br> 
      <li><span class="badge badge-2021" style="width: 85px">12/2021</span> Starting my fixed-term contract as an External AI Research Engineer for the Huawei Paris Research Center </li>
      <li><span class="badge badge-2021" style="width: 85px">05/2021</span> Starting my summer internship as a Research Scientist at <b><a href="https://www.sncf.com/en/innovation-development/innovation-research" target="_blank">SNCF</a></b> </li><br>
	    <li><span class="badge badge-2020" style="width: 85px">06/2020</span> Starting my summer internship as a Machine learning in Finance Researcher at  <b><a href="https://www.cnrs.fr/en" target="_blank">CNRS</a></b> </li>
	    <li><span class="badge badge-2020" style="width: 85px">05/2020</span> I am accepted to the <a href="https://www.ip-paris.fr/en/education/masters/applied-mathematics-and-statistics-program/master-year-2-data-science" target="_blank">Ecole Polytechnique</a> for a Research Master in Machine Learning  </li>
      <li><span class="badge badge-2020" style="width: 85px">05/2020</span> Starting the Applied Statistics Project with <a href="https://www.banque-france.fr/en" target="_blank">Banque de France</a>  </li><br>
      <li><span class="badge badge-2019" style="width: 85px">06/2019</span> Starting my Summer Internship as a Quantitative Analyst at <b><a href="https://www.rothschildandco.com/en/" target="_blank">Rotschild & Co</a></b> </li><br>
	    <li><span class="badge bg-primary" style="width: 85px">09/2018</span> I am accepted to the <a href="https://www.ensae.fr/en" target="_blank">ENSAE PARIS </a>  </li>
	  </ul>
    
	  <br>

        </div>
      </div>


    <div class="row">
        <div class="col-md-12">
          <h3 class="pb-1 mb-3 border-bottom">Publications</h3>
     <!-- Mantis -->
     <div class="row">
      <div class="col-xs-10 col-sm-4 col-md-4">
        <a href="adapters/index.html">
  <img class="img-thumbnail mb-3" src="mantis/mantis.png" alt="Enhancing Multivariate Time Series Forecasting via Multi-Task Learning and Random Matrix Theory">
        </a>
      </div>
      <div class="col-xs-12 col-sm-8 col-md-8" style="font-size: 120%;">
        <strong>Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series Classification</strong><br>
        Vasilii Feofanov, Marius Alonso, Songkang Wen, <u>Romain Ilbert</u>, Hongbo Guo, Malik Tiomoko, Lujia Pan, Jianfeng Zhang and Ievgen Redko<br>
        <i>ICML Workshop on Foundation Models for Structured Data</i> <br>
    <br>
        <a target="_blank" href="https://openreview.net/forum?id=FDDkR53rim"> <button type="button" class="btn btn-primary btn-sm"> Paper </button></a>
        <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract24">Abstract</button>
        <div id="abstract24" class="collapse">
  <p class="bg-light">
    In recent years, there has been increasing interest in developing foundation models for time series data that can generalize across diverse downstream tasks. 
    While numerous forecasting-oriented foundation models have been introduced, there is a notable scarcity of models tailored for time series classification. 
    To address this gap, we present Mantis, a new open-source foundation model for time series classification based on the Vision Transformer (ViT) architecture that has been pre-trained using a contrastive learning approach. 
    Our experimental results show that Mantis outperforms existing foundation models both when the backbone is frozen and when fine-tuned, while achieving the lowest calibration error. 
    In addition, we propose several adapters to handle the multivariate setting, reducing memory requirements and modeling channel interdependence.
  </p>
        </div>
        <div style="height:30px;"></div>
      </div>
     </div>
     <!-- Enhancing -->
     <div class="row">
      <div class="col-xs-10 col-sm-4 col-md-4">
        <a href="multitask/index.html">
  <img class="img-thumbnail mb-3" src="enhancing/image.png" alt="Analysing Multi-Task Regression via Random Matrix Theory with Application to Time Series Forecasting">
        </a>
      </div>
      <div class="col-xs-12 col-sm-8 col-md-8" style="font-size: 120%;">
        <strong>Enhancing Multivariate Time Series Forecasting via Multi-Task Learning and Random Matrix Theory</strong><br>
        <u>Romain Ilbert</u>, Malik Tiomoko, Cosme Louart, Vasilii Feofanov, Themis Palpanas and Ievgen Redko<br>
        <i>NeurIPS Workshop "Time Series in the Age of Large Models"</i> <br>
    <br>
        <a target="_blank" href="https://openreview.net/forum?id=YaErRZ5SVG&referrer=%5Bthe%20profile%20of%20Romain%20Ilbert%5D(%2Fprofile%3Fid%3D~Romain_Ilbert1)"> <button type="button" class="btn btn-primary btn-sm"> Paper </button></a>
        <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract23">Abstract</button>
        <div id="abstract23" class="collapse">
  <p class="bg-light">
    We present a novel approach to multivariate time series forecasting by framing it as a multi-task learning problem. 
    We propose an optimization strategy that enhances single-channel predictions by leveraging information across multiple channels. 
    Our framework offers a closed-form solution for linear models and connects forecasting performance to key statistical properties using advanced analytical tools. 
    Empirical results on both synthetic and real-world datasets demonstrate that integrating our method into training loss functions significantly improves univariate models by effectively utilizing multivariate data within a multi-task learning framework.
  </p>
        </div>
        <div style="height:30px;"></div>
      </div>
     </div>
     <!-- Adapters -->
     <div class="row">
      <div class="col-xs-10 col-sm-4 col-md-4">
        <a href="adapters/index.html">
  <img class="img-thumbnail mb-3" src="adapters/adapters.png" alt="Enhancing Multivariate Time Series Forecasting via Multi-Task Learning and Random Matrix Theory">
        </a>
      </div>
      <div class="col-xs-12 col-sm-8 col-md-8" style="font-size: 120%;">
        <strong>User-friendly Foundation Model Adapters for Multivariate Time Series Classification</strong><br>
        <u>Romain Ilbert</u>, Vasilii Feofanov, Malik Tiomoko, Themis Palpanas and Ievgen Redko<br>
        <i>ICDE Workshop on Multivariate Time Series Analytics (an A&#42; conference)</i> <br>
    <br>
        <a target="_blank" href="adapters/adapters_multisa.pdf"> <button type="button" class="btn btn-primary btn-sm"> Paper </button></a>
        <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract22">Abstract</button>
        <div id="abstract22" class="collapse">
  <p class="bg-light">
    Foundation models, while highly effective, are often resource-intensive, requiring substantial inference time and memory. 
    This paper addresses the challenge of making these models more accessible with limited computational resources by exploring dimensionality reduction techniques. 
    Our goal is to enable users to run large pre-trained foundation models on standard GPUs without sacrificing performance. 
    We investigate classical methods such as Principal Component Analysis alongside neural network-based adapters, aiming to reduce the dimensionality of multivariate time series data while preserving key features. 
    Our experiments show up to a 10x speedup compared to the baseline model, without performance degradation, and enable up to 4.5x more datasets to fit on a single GPU, paving the way for more user-friendly and scalable foundation models.
  </p>
        </div>
        <div style="height:30px;"></div>
      </div>
     </div>
     <!-- Multitask -->
     <div class="row">
      <div class="col-xs-10 col-sm-4 col-md-4">
        <a href="multitask/index.html">
  <img class="img-thumbnail mb-3" src="multitask/image.png" alt="Analysing Multi-Task Regression via Random Matrix Theory with Application to Time Series Forecasting">
        </a>
      </div>
      <div class="col-xs-12 col-sm-8 col-md-8" style="font-size: 120%;">
        <strong>Analysing Multi-Task Regression via Random Matrix Theory with Application to Time Series Forecasting</strong><br>
        <u>Romain Ilbert</u>, Malik Tiomoko, Cosme Louart, Ambroise Odonnat, Vasilii Feofanov, Themis Palpanas and Ievgen Redko<br>
        <i>NeurIPS 2024 (Spotlight)</i> <br>
    <br>
        <a target="_blank" href="https://openreview.net/forum?id=FFW6rPz48Z&referrer=%5Bthe%20profile%20of%20Ambroise%20Odonnat%5D(%2Fprofile%3Fid%3D~Ambroise_Odonnat1)"> <button type="button" class="btn btn-primary btn-sm"> Paper </button></a>
        <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract20">Abstract</button>
        <div id="abstract20" class="collapse">
  <p class="bg-light">
      In this paper, we introduce a novel theoretical framework for multi-task regression, applying random matrix theory to provide precise performance estimations, under high-dimensional, non-Gaussian data distributions. 
      We formulate a multi-task optimization problem as a regularization technique to enable single-task models to leverage multi-task learning information. 
      We derive a closed-form solution for multi-task optimization in the context of linear models. Our analysis provides valuable insights by linking the multi-task learning performance to various model statistics such as raw data covariances, signal-generating hyperplanes, noise levels, as well as the size and number of datasets. 
      We finally propose a consistent estimation of training and testing errors, thereby offering a robust foundation for hyperparameter optimization in multi-task regression scenarios. 
      Experimental validations on both synthetic and real-world datasets in regression and multivariate time series forecasting demonstrate improvements on univariate models, incorporating our method into the training loss and thus leveraging multivariate information.
  </p>
        </div>
        <div style="height:30px;"></div>
      </div>
     </div>
     <!-- SAMformer-->
     <div class="row">
            <div class="col-xs-10 col-sm-4 col-md-4">
              <a href="samformer/index.html">
        <img class="img-thumbnail mb-3" src="samformer/image.png" alt="SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention">
              </a>
            </div>
            <div class="col-xs-12 col-sm-8 col-md-8" style="font-size: 120%;">
              <strong>SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention</strong><br>
          <u>Romain Ilbert</u>, Ambroise Odonnat, Vasilli Feofanov, Aladin Virmaux, Giuseppe Paolo, Themis Palpanas and Ievgen Redko<br>
          <i>ICML 2024 (Oral)</i> <br>
              <a target="_blank" href="https://openreview.net/forum?id=8kLzL5QBh2&referrer=%5Bthe%20profile%20of%20Vasilii%20Feofanov%5D(%2Fprofile%3Fid%3D~Vasilii_Feofanov1)"> <button type="button" class="btn btn-primary btn-sm"> Paper </button></a>
          <a target="_blank" href="https://github.com/romilbert/samformer"> <button type="button" class="btn btn-primary btn-sm"> Code </button></a>
              <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#bibtex21">BibTex</button>
              <div id="bibtex21" class="collapse">
        <pre><tt>@inproceedings{
            ilbert2024samformer,
            title={{SAM}former: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention},
            author={Romain Ilbert and Ambroise Odonnat and Vasilii Feofanov and Aladin Virmaux and Giuseppe Paolo and Themis Palpanas and Ievgen Redko},
            booktitle={Forty-first International Conference on Machine Learning},
            year={2024},
            url={https://openreview.net/forum?id=8kLzL5QBh2}
            }</tt></pre>
              </div>
              <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract21">Abstract</button>
              <div id="abstract21" class="collapse">
        <p class="bg-light">
            Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. 
            To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. 
            We further identify the attention of transformers as being responsible for this low generalization capacity. 
            Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. 
            We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. 
            In particular, SAMformer surpasses current state-of-the-art methods and is on par with the biggest foundation model MOIRAI while having significantly fewer parameters. 
            The code is available at https://github.com/romilbert/samformer.
        </p>
              </div>
              <div style="height:30px;"></div>
            </div>
          </div>
    
      <!-- Augmentation -->
      <div class="row">
            <div class="col-xs-10 col-sm-4 col-md-4">
        <img class="img-thumbnail mb-3" src="augmentation/image.png" alt="Data Augmentation for Multivariate Time Series Classification: An Experimental Study">
              </a>
            </div>
            <div class="col-xs-12 col-sm-8 col-md-8" style="font-size: 120%;">
              <strong>Data Augmentation for Multivariate Time Series Classification: An Experimental Study</strong><br>
          <u>Romain Ilbert</u>, Thai V. Hoang, Zonghua Zhang <br>
          <i>ICDE Workshop on Multivariate Time Series Analytics (an A&#42; conference)</i> <br>
          <a target="_blank" href="https://arxiv.org/abs/2406.06518"> <button type="button" class="btn btn-primary btn-sm"> Paper </button></a>
              <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#bibtex19">BibTex</button>
              <div id="bibtex19" class="collapse">
        <pre><tt>@misc{ilbert2024data,
        title={Data Augmentation for Multivariate Time Series Classification: An Experimental Study}, 
        author={Romain Ilbert and Thai V. Hoang and Zonghua Zhang},
        year={2024},
        eprint={2406.06518},
        archivePrefix={arXiv},
        primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
    }
    }</tt></pre>
              </div>
              <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract19">Abstract</button>
              <div id="abstract19" class="collapse">
        <p class="bg-light">
            Our study investigates the impact of data augmentation on the performance of multivariate time series models, focusing on datasets from the UCR archive. 
            Despite the limited size of these datasets, we achieved classification accuracy improvements in 10 out of 13 datasets using the Rocket and InceptionTime models. 
            This highlights the essential role of sufficient data in training effective models, paralleling the advancements seen in computer vision. 
            Our work delves into adapting and applying existing methods in innovative ways to the domain of multivariate time series classification. 
            Our comprehensive exploration of these techniques sets a new standard for addressing data scarcity in time series analysis, emphasizing that diverse augmentation strategies are crucial for unlocking the potential of both traditional and deep learning models. 
            Moreover, by meticulously analyzing and applying a variety of augmentation techniques, we demonstrate that strategic data enrichment can enhance model accuracy. 
            This not only establishes a benchmark for future research in time series analysis but also underscores the importance of adopting varied augmentation approaches to improve model performance in the face of limited data availability.
        </p>
              </div>
              <div style="height:30px;"></div>
            </div>
          </div>
    
      <!-- Boundaries -->
      <div class="row">
            <div class="col-xs-10 col-sm-4 col-md-4">
        <img class="img-thumbnail mb-3" src="boundaries/image.png" alt="Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting">
              </a>
            </div>
            <div class="col-xs-12 col-sm-8 col-md-8" style="font-size: 120%;">
              <strong>Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting</strong><br>
          <u>Romain Ilbert</u>, Thai V. Hoang, Zonghua Zhang and Themis Palpanas <br>
          <i>ACM-CCS Workshop on Recent Advances in Resilient and Trustworthy ML Systems in Autonomous Networks (an A&#42; conference in cybersecurity)</i> <br>
          <a target="_blank" href="https://arxiv.org/abs/2311.09790"> <button type="button" class="btn btn-primary btn-sm"> Paper </button></a>
              <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#bibtex18">BibTex</button>
              <div id="bibtex18" class="collapse">
        <pre><tt>@inproceedings{10.1145/3605772.3624002, 
            author = {Ilbert, Romain and Hoang, Thai V. and Zhang, Zonghua and Palpanas, Themis}, 
            title = {Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting}, 
            year = {2023}, 
            isbn = {9798400702655}, 
            publisher = {Association for Computing Machinery}, 
            address = {New York, NY, USA}, 
            url = {https://doi.org/10.1145/3605772.3624002}, 
            doi = {10.1145/3605772.3624002}, 
            abstract = {Balancing the trade-off between accuracy and robustness is a long-standing challenge in time series forecasting. While most of existing robust algorithms have achieved certain suboptimal performance on clean data, sustaining the same performance level in the presence of data perturbations remains extremely hard. In this paper, we study a wide array of perturbation scenarios and propose novel defense mechanisms against adversarial attacks using real-world telecom data. We compare our strategy against two existing adversarial training algorithms under a range of maximal allowed perturbations, defined using ell_infty -norm, in [0.1,0.4]. Our findings reveal that our hybrid strategy, which is composed of a classifier to detect adversarial examples, a denoiser to eliminate noise from the perturbed data samples, and a standard forecaster, achieves the best performance on both clean and perturbed data. Our optimal model can retain up to 92.02\% the performance of the original forecasting model in terms of Mean Squared Error (MSE) on clean data, while being more robust than the standard adversarially trained models on perturbed data. Its MSE is 2.71\texttimes{} and 2.51\texttimes{} lower than those of comparing methods on normal and perturbed data, respectively. In addition, the components of our models can be trained in parallel, resulting in better computational efficiency. Our results indicate that we can optimally balance the trade-off between the performance and robustness of forecasting models by improving the classifier and denoiser, even in the presence of sophisticated and destructive poisoning attacks.}, 
            booktitle = {Proceedings of the 2023 Workshop on Recent Advances in Resilient and Trustworthy ML Systems in Autonomous Networks}, 
            pages = {17â€“28}, 
            numpages = {12}, 
            keywords = {robustness, poisoning, performance, forecasting, denoising, components, classification}, 
            location = {, Copenhagen, Denmark, }, 
            series = {ARTMAN '23} }
    }</tt></pre>
              </div>
              <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract18">Abstract</button>
              <div id="abstract18" class="collapse">
        <p class="bg-light">
            Balancing the trade-off between accuracy and robustness is a long-standing challenge in time series forecasting. While most of existing robust algorithms have achieved certain suboptimal performance on clean data, sustaining the same performance level in the presence of data perturbations remains extremely hard. In this paper, we study a wide array of perturbation scenarios and propose novel defense mechanisms against adversarial attacks using real-world telecom data. We compare our strategy against two existing adversarial training algorithms under a range of maximal allowed perturbations, defined using â„“âˆž-norm, âˆˆ[0.1,0.4]. Our findings reveal that our hybrid strategy, which is composed of a classifier to detect adversarial examples, a denoiser to eliminate noise from the perturbed data samples, and a standard forecaster, achieves the best performance on both clean and perturbed data. Our optimal model can retain up to 92.02% the performance of the original forecasting model in terms of Mean Squared Error (MSE) on clean data, while being more robust than the standard adversarially trained models on perturbed data. Its MSE is 2.71Ã— and 2.51Ã— lower than those of comparing methods on normal and perturbed data, respectively. In addition, the components of our models can be trained in parallel, resulting in better computational efficiency. Our results indicate that we can optimally balance the trade-off between the performance and robustness of forecasting models by improving the classifier and denoiser, even in the presence of sophisticated and destructive poisoning attacks.
        </p>
              </div>
              <div style="height:30px;"></div>
            </div>
          </div>
    </div>

    </div> <!-- /container -->

      <div class="container">
        <footer>
          <p><small>Copyright &#169; Romain Ilbert &nbsp;/&nbsp; Last update June 2025 <br>
	      Inspired from the personal page of <a href="https://Mathis.Petrovich.fr">Mathis Petrovich</a>
	  </small></p>

        </footer>
        <div style="height:10px;"></div>
      </div>

    <!-- Bootstrap core JS-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Core theme JS-->
    <script src="js/scripts.js"></script>

  </body>
</html>


